import streamlit as st
import pandas as pd
import json
import glob
import os
from datetime import datetime, date

# Importa√ß√µes locais do projeto
from src import main as main_scrapper
from src.models import Config, AdvancedMatchRule, ScheduleConfig, LoggingConfig, StorageConfig
from src import config

st.set_page_config(
    page_title="Relat√≥rio do Monitor DOU",
    page_icon="üóûÔ∏è",
    layout="wide"
)

def load_data(data_dir="data"):
    """
    Carrega todos os arquivos JSONL do diret√≥rio de dados e os agrega por URL do artigo.
    Retorna:
        list[dict]: Lista de artigos √∫nicos com suas correspond√™ncias.
    """
    articles_map = {}
    
    # Encontra todos os arquivos jsonl
    files = glob.glob(os.path.join(data_dir, "*.jsonl"))
    
    for file_path in files:
        filename = os.path.basename(file_path)
        keyword_slug = filename.replace(".jsonl", "")
        
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    url = entry.get("url")
                    
                    if not url:
                        continue
                        
                    if url not in articles_map:
                        articles_map[url] = {
                            "title": entry.get("title", "Sem T√≠tulo"),
                            "url": url,
                            "date": entry.get("date"),
                            "section": entry.get("section"),
                            "matches": []
                        }
                    
                    # Adiciona detalhes da correspond√™ncia
                    match_info = {
                        "keyword": entry.get("keyword"),
                        "context": entry.get("context"),
                        "source_file": filename
                    }
                    articles_map[url]["matches"].append(match_info)
                    
                except json.JSONDecodeError:
                    continue
                    
    return list(articles_map.values())

def run_custom_search_view():
    st.title("üîç Busca Personalizada no DOU")
    
    with st.form("search_form"):
        col1, col2 = st.columns(2)
        with col1:
            search_date = st.date_input("Data da Busca", value=date.today())
        with col2:
            search_sections = st.multiselect("Se√ß√µes", ["dou1", "dou2", "dou3"], default=["dou3"])
        
        # Advanced Rules inputs
        st.subheader("Crit√©rios de Busca")
        st.info("Preencha pelo menos um campo de termos.")
        
        title_terms_str = st.text_input("Termos no T√≠tulo (separados por v√≠rgula)")
        body_terms_str = st.text_input("Termos no Corpo (separados por v√≠rgula)")
        
        submitted = st.form_submit_button("Executar Busca")
    
    if submitted:
        if not title_terms_str and not body_terms_str:
            st.error("Por favor adicione termos de busca para T√≠tulo ou Corpo.")
            return
        
        # Construct Config
        title_terms = [t.strip() for t in title_terms_str.split(",") if t.strip()]
        body_terms = [t.strip() for t in body_terms_str.split(",") if t.strip()]
        
        rule = AdvancedMatchRule(
            name="Busca Manual",
            title_terms=title_terms,
            body_terms=body_terms
        )
        
        # Create a temporary config
        # We use dummy values for logging/schedule as they are not used in search directly
        temp_config = Config(
            schedule=ScheduleConfig(time="00:00"),
            logging=LoggingConfig(),
            storage=StorageConfig(),
            keywords=[], # No simple keywords, using rules
            rules=[rule],
            sections=search_sections
        )
        
        with st.spinner("Executando raspagem... isso pode levar alguns segundos."):
            try:
                matches = main_scrapper.run_scraper(temp_config, search_date)
                
                if not matches:
                    st.warning("Nenhuma correspond√™ncia encontrada com os crit√©rios informados.")
                else:
                    # Agrega correspond√™ncias por URL para evitar duplicatas visuais
                    unique_articles = {}
                    for match in matches:
                        if match.url not in unique_articles:
                            unique_articles[match.url] = {
                                "title": match.title,
                                "section": match.section,
                                "url": match.url,
                                "contexts": []
                            }
                        unique_articles[match.url]["contexts"].append(match.context)
                    
                    st.success(f"Encontradas {len(matches)} correspond√™ncias em {len(unique_articles)} publica√ß√µes!")

                    # Exibe resultados por artigo √∫nico
                    st.divider()
                    for url, article in unique_articles.items():
                        with st.container():
                            st.subheader(f"{article['title']} (Se√ß√£o {article['section']})")
                            st.markdown(f"[{article['url']}]({article['url']})")
                            
                            with st.expander(f"Ver {len(article['contexts'])} trecho(s) encontrado(s)", expanded=True):
                                for ctx in article['contexts']:
                                    st.markdown(f"> {ctx}")
                                    st.markdown("---")
                            st.divider()
                            
            except Exception as e:
                st.error(f"Erro ao executar a busca: {str(e)}")


def run_daily_report_view():
    st.title("üóûÔ∏è Monitor DOU: Relat√≥rio Di√°rio")
    
    # --- Barra Lateral ---
    st.sidebar.header("Filtros")
    
    # Carrega Dados
    if not os.path.exists("data"):
        st.error("Diret√≥rio de dados n√£o encontrado. Por favor, execute o raspador primeiro.")
        return

    all_articles = load_data()
    
    if not all_articles:
        st.info("Nenhum dado encontrado.")
        return

    # Converte para DF para filtrar metadados mais facilmente
    df_meta = pd.DataFrame([
        {
            "url": a["url"], 
            "date": a["date"], 
            "section": a["section"], 
            "match_count": len(a["matches"])
        } 
        for a in all_articles
    ])
    
    if df_meta.empty:
        st.info("Nenhum dado encontrado nos arquivos.")
        return
    
    # Filtro: Data
    available_dates = sorted(df_meta["date"].unique(), reverse=True)
    if not available_dates:
         st.warning("Sem datas dispon√≠veis.")
         return

    selected_date = st.sidebar.selectbox("Selecionar Data", available_dates)
    
    # Filtro: Se√ß√£o
    available_sections = sorted(df_meta["section"].unique())
    selected_sections = st.sidebar.multiselect(
        "Selecionar Se√ß√µes", 
        available_sections, 
        default=available_sections
    )
    
    # Aplica Filtros
    filtered_articles = [
        a for a in all_articles 
        if a["date"] == selected_date and a["section"] in selected_sections
    ]
    
    # M√©tricas
    total_articles = len(filtered_articles)
    total_matches = sum(len(a["matches"]) for a in filtered_articles)
    
    c1, c2, c3 = st.columns(3)
    c1.metric("Data", selected_date)
    c2.metric("Artigos Encontrados", total_articles)
    c3.metric("Total de Correspond√™ncias", total_matches)
    
    st.markdown("---")
    
    # --- Conte√∫do Principal ---
    for article in filtered_articles:
        with st.container():
            # Cabe√ßalho com T√≠tulo e Emblemas
            col_title, col_badges = st.columns([5, 2])
            
            with col_title:
                st.subheader(article["title"])
                st.markdown(f"[{article['url']}]({article['url']})")
                
            with col_badges:
                st.caption(f"Se√ß√£o: {article['section']}")
                matches_count = len(article['matches'])
                color = "red" if matches_count > 0 else "grey"
                st.markdown(f":{color}[Correspond√™ncias: {matches_count}]")

            # Expansor para Contextos
            with st.expander("Ver Contexto das Correspond√™ncias"):
                for i, match in enumerate(article["matches"]):
                    st.markdown(f"**Correspond√™ncia #{i+1}** - Palavra-chave: `{match['keyword']}`")
                    context = match['context']
                    st.markdown(f"> {context}")
                    st.divider()

def main():
    st.sidebar.title("Navega√ß√£o")
    # Usa radio ou selectbox para navega√ß√£o
    page = st.sidebar.radio("Ir para", ["Relat√≥rio Di√°rio", "Busca Personalizada"])
    
    if page == "Relat√≥rio Di√°rio":
        run_daily_report_view()
    elif page == "Busca Personalizada":
        run_custom_search_view()

if __name__ == "__main__":
    main()

